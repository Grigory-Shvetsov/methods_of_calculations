\documentclass[12pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[russian]{babel}
\usepackage[oglav,spisok,boldsect, figwhole]{./style/fn2kursstyle1}
\graphicspath{{./style/}{./figures/}}
%\usepackage{float}%"Плавающие" картинки
\usepackage{multirow}
\usepackage{subcaption}
%Римские цифры
\newcommand{\RomanNumeralCaps}[1]
{\MakeUppercase{\romannumeral #1}}

\usepackage{enumitem} 

\usepackage{comment}

%Параметры титульника
\title{Методы решения проблемы собственных значений}
\group{ФН2-52Б}
\author{Г.А.~Швецов}
\supervisor{А.О.~Гусев}
\date{2022}
\begin{document}
	\newcommand{\pl}{\partial}
	\maketitle
	
	\tableofcontents
	
	\newpage
	
	\section{Контрольные вопросы}
	\begin{enumerate}
		\item \textit{Почему нельзя находить собственные числа матрицы $A$, прямо решая уравнение $\det (A-\lambda E) = 0$, а собственные векторы --- «по определению», решая систему $(A - \lambda_j E)e_j =0$?}
		
		Решение характеристического уравнения $\det (A-\lambda E) = 0$ для получения собственных чисел является сложным процессом с точки зрения вычислений. При вычислении собственных векторов путем решения системы $(A - \lambda_j E)e_j =0$ нам известно не точное значение собственного число $\lambda_j$, а лишь некоторое его приближение $\lambda_j^*$. Матрица $(A - \lambda_j^* E)$ оказывается невырожденной и, следовательно, при решении соответствующей системы будем получать только тривиальное решение.
		\smallskip
		
		\item \textit{Докажите, что ортогональное преобразование подобия сохраняет симметрию матрицы.}
		
		Пусть $A$ --- симметричная матрица, т.е. $A^\textrm{T} = A$; $P$ --- матрица ортогонального преобразования (из этого следует, что $P^{-1} = P^\textrm{T}$). Нужно доказать, что матрица $R = P^{-1} A P$ ---  симметричная матрица.
		\[
		R^{\textrm{T}} = (P^{-1} A P)^\textrm{T} = P^\textrm{T} (P^{-1} A)^\textrm{T} = P^{-1} A^\textrm{T} (P^{-1})^\textrm{T} = P^{-1} A P = R,
		\]
		т.е. $R$ --- симметричная матрица.
		
		\smallskip
		
		
		\item \textit{Как преобразование подобия меняет собственные векторы матрицы?}
		
		Сначала заметим, что при преобразовании подобия собственные значения не изменяются. Пусть $R = P^{-1} A P$. Характеристическое многочлен для этой матрицы:
		\begin{eqnarray*}
			&& \det(R - \lambda E) = \det(P^{-1} A P - \lambda E) = \det(P^{-1} A P - P^{-1}(\lambda E) P) = \\
			&& = \det(P^{-1}(A - \lambda E) P) = \det(P^{-1}) \det(A - \lambda E) \det(P) = \det(A - \lambda E).
		\end{eqnarray*}
		
		Характеристические многочлены исходной и новой матрицы совпадают, из чего следует равенство собственных значений. Теперь рассмотрим задачу поиска собственных векторов $x'$ матрицы $R$:
		\[
		R x' = \lambda x', \quad P^{-1} A P x' = \lambda x', \quad A P x' = \lambda P x'.
		\]
		
		Из условия, что собственные вектора $x$ матрицы $A$ удовлетворяют условию $A x = \lambda x$, получаем выражение, связывающее собственные вектора обоих матриц: $x = P x'$.
		
		\smallskip
		
		\item \textit{Почему на практике матрицу $A$ подобными преобразованиями вращения приводят только к форме Хессенберга, но не к треугольному виду?}
		
		При $QR$-разложении верхнетреугольной матрицы $A_k$, полученная на $k$-й итерации, матрица $Q$ будет диагональной, и на ее диагонали будут числа $1$ и $-1$. Диагональные элементы результата каждой итерации $A_{k+1}$ будут совпадать по модулю с соответствующими элементы матрицы $A_k$. Вследствие этого итерационный метод не будет сходится.
		\smallskip
		
		\item \textit{Оцените количество арифметических операций, необходимое для приведения произвольной квадратной матрицы $A$ к форме Хессенберга.}
		
		В программной реализации алгоритма идет цикл $i = \overline{2, n-1}$. В этот цикл вложен другой цикл $j=\overline{i+1, n}$. В этом же цикл тратится 5 операций на вычисление коэффициентов матрицы $T_{ij}$, а также происходят два умножения матрицы $A$ на матрицу $T_{ij}$ (слева и справа). В общем случае умножение на матрицу $T_{ij}$ можно выполнить за $4 n$ операций: изменяются две строки или два столбца с номерами $i$ и $j$, на вычисление каждого элемента требуется 2 операции умножения. Для умножения же $A$ на $T_{ij}$ можно сократить количество операций, если учесть, что в процессе работы идет зануление элементов под диагональю, которая находится под главной диагональю. Тогда умножение можно осуществить за $4(n-i-1)$ умножений. Итоговое количество операций равно
		\[
		\sum_{i=2}^{n-1} \sum_{j=i+1}^n (5 + 4n + 4(n-i-1)) = \dots = \frac{10}{3} n^3 - \frac{23}{2} n^2 + \frac{67}{6}n - 3 \approx \frac{10}{3} n^3.
		\]
		
		\smallskip
		
		\item \textit{Сойдется ли алгоритм обратных итераций, если в качестве начального приближения взять собственный вектор, соответствующий другому собственному значению? Что будет в этой ситуации в методе обратной итерации, использующем отношение Рэлея?}
		
		Из собственных векторов матрицы $A$ можно составить ОНБ $e_i, i = \overline{1, n}$. Пусть $x = \sum_{i=1}^n \alpha_i e_i$, $y = \sum_{i=1}^n \beta_i e_i$. Запишем для этих векторов метод обратных итераций при условии, что нам известно приближение $\lambda_j^*$ собственного значения $\lambda_j$:
		\begin{eqnarray*}
			& (A-\lambda_j^*) y = x;\\
			& \sum_{i=1}^n (A-\lambda_j^*) \beta_i e_i = \sum_{i=1}^n \alpha_i e_i;\\
			& \sum_{i=1}^n \beta_i (\lambda_i - \lambda_j^*) e_i = \sum_{i=1}^n \alpha_i e_i; \\
			& \beta_i (\lambda_i - \lambda_j^*) = \alpha_i \; \forall i; \\
			& \beta_i = \dfrac{\alpha_i}{\lambda_i - \lambda_j^*} \; \forall i.
		\end{eqnarray*}
		
		Получаем, что
		\begin{multline*}
			y = \sum_{i=1}^n \frac{\alpha_i}{\lambda_i - \lambda_j^*} e_i = \frac{\alpha_j}{\lambda_j - \lambda_j^*} e_j + \sum_{i=1, i \neq j}^n \frac{\alpha_i}{\lambda_i - \lambda_j^*} e_i = \\ = \frac1{\lambda_j - \lambda_j^*} \left(\alpha_j e_j + \sum_{i=1, i \neq j}^n \frac{\lambda_j - \lambda_j^*}{\lambda_i - \lambda_j^*} \alpha_i e_i\right).
		\end{multline*}
		
		Заметим, что при условии $|\lambda_j - \lambda_j^*| \ll |\lambda_i - \lambda_j^*|$, то слагаемые под знаком суммы малы по сравнению с $\alpha_j e_j$. По этой причине метод будет сходится к собственному вектору $e_j$, даже если взять в качестве начального приближения взять собственный вектор, отвечающий другому собственному числу.
		
		Т.к. соотношение Рэлея позволяет вычислять приближение собственного значения на основе приближения собственного вектора, то в случае выбора в качестве начального приближения собственного вектора, отвечающему другому собственному числу, метод обратной итерации на основе соотношения Рэлея сойдется к этому самому другому собственному значению.
		
		\smallskip
		
		\item \textit{Сформулируйте и обоснуйте критерий останова для $QR$-алгоритма отыскания собственных значений матрицы.}
		
		Алгоритм поиска собственных значений методом $QR$-разложение составляет последовательность матриц $A_k$, сходящуюся к верхнетреугольной матрице, на диагонали которой стоят собственные значения. При этом изначально берется матрица Хессенберга и на каждой итерации $A_k$ тоже является матрицей Хессенберга. По определению матрицы Хессенберга $H$ $h_{ij} = 0$ при $i > j+1$. Используя данное свойство достаточно проверять не все элементы нижней строки $a_{ni}, i = \overline{1, n-1}$, а только элемент $a_{n,n-1}$. Таким образом, в качестве критерия останова итерационного алгоритма можно использовать критерий $|a_{n, n-1}| < \varepsilon$. После выполнения этого условия можно перейти к задаче поиска собственных чисел матрицы размером $(n-1)\times (n-1)$.
		
		\smallskip
		
		\item \textit{Предложите возможные варианты условий перехода к алгоритму со сдвигами. Предложите алгоритм выбора величины сдвига.}
		
		Переходить к алгоритму со сдвигами следует в том случае, если есть близки по модулю собственные числа. В этом случае сходимость метода будет очень медленной.
		
		Используя круги Гершгорина, можно оценить по модулю собственные числа матрицы. Если по этой оценке получились близки собственные значения, то рекомендуется переходить к алгоритму со сдвигами.
		
		Также если для матриц $A^{(k)}$ и $A^{(k-1)}$ для элементов ниже главной диагонали ($i > j$) выполняется $\sfrac{|a_{ij}^{(k)}|}{|a_{ij}^{(k-1)}|} \approx 1$, т.е. эти элементы матрицы изменяются незначительно, то следует переходить к алгоритму со сдвигами.
		
		В качестве величины сдвига можно можно использовать $a_{nn}^{(k)}$.
		
		\smallskip
		
		\item \textit{Для чего нужно на каждой итерации нормировать приближение к собственному вектору?}
		
		Если $|\lambda| > 1$, то последовательность норм векторов приближений собственного вектора экспоненциально стремится к бесконечности. Если же $|\lambda| < 1$, то эта последовательность также экспоненциально стремится к нулю. В силу этого рационально на каждой итерации нормировать приближение собственного вектора.
		\smallskip
		
		
		\item \textit{Приведите примеры использования собственных чисел	и собственных векторов в численных методах.}
		
		\begin{enumerate}[label=(\arabic*)]
			% Вроде есть в Бортаковский, Пантелеев, "Линейная алгебра в примерах и задачах", стр. 348 (где матрица с коэффицинтами Тейлора)
			% Блочная матрица умножить на блочную матрицу = блочная матрица
			\item Приведение матрицы к жордановой нормальной форме, диагональными элементами которой являются собственные числа, позволяет более эффективно возводить матрицу в степень.
			% Из Амосова
			%\item Собственные числа и собственные векторы являются важнейшими характеристиками, отражающими существенные стороны линейных моделей
			% Из Амосова
			\item В электрических и механических системах собственные числа отвечают частотам колебаний, а собственные векторы характеризуют соответствующие формы (моды) колебаний.
			% Из Амосова
			\item На информации о собственных значениях и собственных векторах матриц основана оценка величин критических нагрузок при расчете строительных конструкций.
			\item Знание собственных значений матрицы позволяет сделать вывод об устойчивости нулевого решения системы линейных дифференциальных уравнений с постоянными коэффициентами.
			\item Отношение максимального и минимального по модулю собственных значений определяет число обусловленности соответствующей матрицы.
			\item Собственные вектора являются главными направлениями кривых второго порядка. Также собственные числа позволяют определить к какому типу относится кривая второго порядка: к параболическому, гиперболическому или эллиптическому.
		\end{enumerate}
		\smallskip
\item Количество операций.
Приведение матрицы к матрице Хессенберга
\[
\sum_{i=1}^{n-2}((4n+4(n-i+1)+5)(n-i-1)) = \frac{10n^3}{3}-\frac{11n^2}{2}-\frac{41n}{6}+9.
\]
Разложение матрицы $A = RQ$ на каждой итерации
\[
\sum_{i=0}^{n-2}(4n+4(n-i+1)+5) = 6n^2+7n-13.
\]
Таким образом получаем
\[
\frac{10n^3}{3}-\frac{11n^2}{2}-\frac{41n}{6}+9+6n^2+7n-13 = \frac{10n^3}{3}+\frac{n^2}{2}+\frac{n}{6}-4
\]
мультипликативных операций.

	\end{enumerate}
	\newpage
	\section{Результаты}
	
	\begin{center}
	\large{Точность $\varepsilon = \textrm{1e-3}$.}
	\end{center}

	
	\subsection{Вариант 1}
	\[
	\large A = \begin{pmatrix}
		28.60 & -7.89 & 2.66 & 6.64 \\
		-7.89 & 37.00 & 8.63 & -7.74 \\
		2.66 & 8.63 & 52.80 & -2.11 \\
		6.64 & -7.74 & -2.11 & 0.80
	\end{pmatrix}
	\]
	\\
	Точный результат (вычисленный в Wolfram Mathematica):
	\begin{eqnarray*}
		& \lambda_1 = 57.165516324888 \qquad \Longleftrightarrow \qquad e_1 = \begin{pmatrix}
			-0.063563494887\\ 0.443928222700\\ 0.888001562853\\-0.101688935383
		\end{pmatrix};\\\\
		& \lambda_2 = 41.343846802611 \qquad \Longleftrightarrow \qquad e_2 = \begin{pmatrix}
			0.600071540219\\-0.663245923611\\ 0.397904166529\\ 0.204184391401
		\end{pmatrix};\\\\
		& \lambda_3 = 22.359615081837 \qquad \Longleftrightarrow \qquad e_3 = \begin{pmatrix}
			0.777968847720\\ 0.582505207755\\-0.229455791959\\ 0.052935757783
		\end{pmatrix};\\\\
		& \lambda_4 = -1.668978209337 \qquad \Longleftrightarrow \qquad e_4 = \begin{pmatrix}
			-0.175037997915\\ 0.154013833283\\ 0.021806837607\\ 0.972196430911
		\end{pmatrix}.
	\end{eqnarray*}
	\newpage
	
	Последовательность приближений $H^{(k)}$:
	
	\begin{eqnarray*}
		&  H^{(0)} = \begin{pmatrix}
			28.6000	& 10.6498	& 0.0000	& 0.0000 \\
			10.6498	& 27.2128	& 15.0657 &	0.0000	\\
			0.0000	& 15.0657	& 9.9958	& 12.0438 \\
			0.0000	& 0.0000	& 12.0438	& 53.3914
		\end{pmatrix} \\\\
		& H^{(1)} = \begin{pmatrix}
			20.6596	& -9.8357	& -0.0000	& 0.0000\\
			-9.8357	& 12.6257	& 17.4321	& 0.0000\\
			0.0000	& 17.4321	& 28.8825	& 1.6738\\
			0.0000	& 0.0000	& 1.6738	& 57.0323	
		\end{pmatrix} \\\\
		& H^{(2)} = \begin{pmatrix}
			15.1552	& 11.4617	& 0.0000	& 0.0000\\
			11.4617	& 7.5614	& 7.6339	& -0.0000\\
			0.0000	& 7.6339	& 39.3180	& 0.0116\\
			-0.0000	& -0.0000	& 0.0116	& 57.1655	
		\end{pmatrix} \\\\
		& H^{(3)} = \begin{pmatrix}
			8.8081	& -11.9717	& 0.0000	& 0.0000\\
			-11.9717	& 12.1871	& 2.7456	& 0.0000\\
			-0.0000	& 2.7456	& 41.0393	& 0.0000\\
			-0.0000	& 0.0000	& 0.0000	& 57.1655
		\end{pmatrix} \\\\
		& \ldots\ldots\ldots\ldots \\\\
		& H^{(7)} = \begin{pmatrix}
			-1.6690	& -0.0000	& 0.0000	& 0.0000\\
			-0.0000	& 22.3596	& 0.0000	& 0.0000\\
			-0.0000	& 0.0000	& 41.3438	& 0.0000\\
			-0.0000	& 0.0000	& 0.0000	& 57.1655
		\end{pmatrix}
	\end{eqnarray*}
	
	Последовательность приближений к собственным векторам:
	\begin{eqnarray*}
		& \lambda_1 = 57.16551632: \\
		& e_1^{(0)} = \begin{pmatrix}
			1.00000000 \\ 0.00000000 \\ 0.00000000 \\ 0.00000000
		\end{pmatrix} \quad e_1^{(1)} = \begin{pmatrix}
			-0.06356349 \\ 0.44392822 \\ 0.88800156 \\-0.10168894
		\end{pmatrix} \quad e_1^{(2)} = \begin{pmatrix}
			0.06356349 \\-0.44392822 \\-0.88800156 \\ 0.10168894
		\end{pmatrix} \\
		& \lambda_2 = 41.34384680:\\
		& e_2^{(0)} = \begin{pmatrix}
			1.00000000 \\ 0.00000000 \\ 0.00000000 \\ 0.00000000
		\end{pmatrix} \quad e_2^{(1)} = \begin{pmatrix}
			0.60007154 \\-0.66324592 \\ 0.39790417 \\ 0.20418439
		\end{pmatrix} \quad e_2^{(2)} = \begin{pmatrix}
			0.60007154 \\-0.66324592 \\ 0.39790417 \\ 0.20418439
		\end{pmatrix} \\
		& \lambda_3 = 22.35961508:\\
		& e_3^{(0)} = \begin{pmatrix}
			1.00000000 \\ 0.00000000 \\ 0.00000000 \\ 0.00000000
		\end{pmatrix} \quad e_3^{(1)} = \begin{pmatrix}
			0.77796885 \\ 0.58250521 \\-0.22945579 \\ 0.05293576
		\end{pmatrix} \quad e_3^{(2)} = \begin{pmatrix}
			0.77796885 \\ 0.58250521 \\-0.22945579 \\ 0.05293576
		\end{pmatrix} \\
		& \lambda_4 = -1.66897821:\\
		& e_4^{(0)} = \begin{pmatrix}
			1.00000000 \\ 0.00000000 \\ 0.00000000 \\ 0.00000000
		\end{pmatrix} \quad e_4^{(1)} = \begin{pmatrix}
			-0.17503800 \\ 0.15401383 \\ 0.02180684 \\ 0.97219643
		\end{pmatrix} \quad e_4^{(2)} = \begin{pmatrix}
			0.17503800 \\-0.15401383 \\-0.02180684 \\-0.97219643
		\end{pmatrix}
	\end{eqnarray*}
	
	Результаты работы комбинированного метода:
	\begin{eqnarray*}
		& \lambda_1 = -1.66897820: e_1 = \begin{pmatrix}
			-0.17503800	\\ 0.15401383	\\ 0.02180684	\\ 0.97219643
		\end{pmatrix}; \lambda_2 = 22.35961508: e_2 = \begin{pmatrix}
			-0.77796885	\\ -0.58250521	\\ 0.22945579	\\ -0.05293576
		\end{pmatrix}; \\
		& \lambda_3 = 57.16551632: e_3 = \begin{pmatrix}
			-0.06356349	\\ 0.44392822	\\ 0.88800156	\\ -0.10168894
		\end{pmatrix}; \lambda_4 = 41.34384680: e_4 = \begin{pmatrix}
			0.60007154	\\ -0.66324592	\\ 0.39790417	\\ 0.20418439
		\end{pmatrix}.
	\end{eqnarray*}
	
	Оценка эффективности QR-алгоритма
	
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Метод}        & \textbf{{Хессенберг+Сдвиг}}&\textbf{{Хессенберг}}&\textbf{{Сдвиг}}&\textbf{{-}} \\ \hline
			Кол-во итераций &7&37&9&37           \\ \hline
			Кол-во операций &107+507&107+1787&869&2200           \\ \hline
		\end{tabular}
	\end{center}
	
	\subsection{Вариант 20}
	\[
	\large
	A = \begin{pmatrix}
		99.40 & -2.90 & -9.98 & 0.63 \\
		-2.90 & 106.40 & -9.43 & -8.02 \\
		-9.98 & -9.43 & -159.40 & -5.89 \\
		0.63 & -8.02 & -5.89 & 58.20
	\end{pmatrix}
	\]
	\\
	Точный результат (вычисленный в Wolfram Mathematica):
	\begin{eqnarray*}
		& \lambda_1 = -160.298356125032 \qquad \Longleftrightarrow \qquad e_1 = \begin{pmatrix}
			-0.038699472277\\-0.036561095822\\-0.998185292796\\-0.028138201150
		\end{pmatrix}\\
		& \lambda_2 =  108.744492116884 \qquad \Longleftrightarrow \qquad e_2 = \begin{pmatrix}
			0.283298382777\\-0.946791113565\\ 0.019424623695\\ 0.151496856684
		\end{pmatrix}\\
		& \lambda_3 =   99.013350459912 \qquad \Longleftrightarrow \qquad e_3 = \begin{pmatrix}
			0.958175988212\\ 0.280375504121\\-0.046470683202\\-0.033598034245
		\end{pmatrix}\\
		& \lambda_4 =   57.140513548235 \qquad \Longleftrightarrow \qquad e_4 = \begin{pmatrix}
			-0.011964646888\\ 0.153751260924\\-0.033004254926\\ 0.987485754908
		\end{pmatrix}
	\end{eqnarray*}
	
	Последовательность приближений $H^{(k)}$:
	\begin{eqnarray*}
		& H^{(0)} = \begin{pmatrix}
			99.4000	& 10.4119	& 0.0000	& 0.0000\\
			10.4119	& -142.0647	& 66.5965	& -0.0000\\
			-0.0000	& 66.5965	& 88.9314	& 8.0087\\
			0.0000	& 0.0000	& 8.0087	& 58.3333
		\end{pmatrix}\\\\
		& H^{(1)} = \begin{pmatrix}
			89.7767	& 51.0623	& 0.0000	& -0.0000\\
			51.0623	& -148.8974	& -16.1277	& 0.0000\\
			0.0000	& -16.1277	& 106.5795	& 0.1922\\
			0.0000	& 0.0000	& 0.1922	& 57.1412
		\end{pmatrix} \\\\
		& H^{(2)} = \begin{pmatrix}
			-33.3365	& 130.4573 &	0.0000 &	0.0000\\
			130.4573	& -26.1476	& 5.2161	& -0.0000\\
			0.0000	& 5.2161	& 106.9436	& 0.0000\\
			0.0000	& -0.0000	& 0.0000	& 57.1405
		\end{pmatrix} \\\\
		& \ldots\ldots\ldots\ldots \\\\
		& H^{(6)} = \begin{pmatrix}
			-160.2984	& 0.0000	& -0.0000	& 0.0000\\
			0.0000	& 99.0134	& 0.0000	& -0.0000\\
			0.0000	& 0.0000	& 108.7445	& 0.0000\\
			0.0000	& -0.0000	& 0.0000	& 57.1405
		\end{pmatrix}
	\end{eqnarray*}
	
	Последовательность приближений к собственным векторам:
	\begin{eqnarray*}
		& \lambda_1 = 57.14051355:\\
		& e_1^{(0)} = \begin{pmatrix}
			1.00000000 \\ 0.00000000 \\ 0.00000000 \\ 0.00000000
		\end{pmatrix} \quad e_1^{(1)} = \begin{pmatrix}
			-0.01196465 \\ 0.15375126 \\-0.03300425 \\ 0.98748575
		\end{pmatrix} \quad e_1^{(2)} = \begin{pmatrix}
			0.01196465 \\-0.15375126 \\ 0.03300425 \\-0.98748575
		\end{pmatrix} \\
		& \lambda_2 = 108.74449212:\\
		& e_2^{(0)} = \begin{pmatrix}
			1.00000000 \\ 0.00000000 \\ 0.00000000 \\ 0.00000000
		\end{pmatrix} \quad e_2^{(1)} = \begin{pmatrix}
			0.28329838 \\-0.94679111 \\ 0.01942462 \\ 0.15149686
		\end{pmatrix} \quad e_2^{(2)} = \begin{pmatrix}
			0.28329838 \\-0.94679111 \\ 0.01942462 \\ 0.15149686
		\end{pmatrix} \\
		& \lambda_3 = 99.01335046:\\
		& e_3^{(0)} = \begin{pmatrix}
			1.00000000 \\ 0.00000000 \\ 0.00000000 \\ 0.00000000
		\end{pmatrix} \quad e_3^{(1)} = \begin{pmatrix}
			0.95817599 \\ 0.28037550 \\-0.04647068 \\-0.03359803
		\end{pmatrix} \quad e_3^{(2)} = \begin{pmatrix}
			0.95817599 \\ 0.28037550 \\-0.04647068 \\-0.03359803
		\end{pmatrix} \\
		& \lambda_4 = -160.29835613:\\
		& e_4^{(0)} = \begin{pmatrix}
			1.00000000 \\ 0.00000000 \\ 0.00000000 \\ 0.00000000
		\end{pmatrix} \quad e_4^{(1)} = \begin{pmatrix}
			-0.03869947 \\-0.03656110 \\-0.99818529 \\-0.02813820
		\end{pmatrix} \quad e_4^{(2)} = \begin{pmatrix}
			0.03869947 \\ 0.03656110 \\ 0.99818529 \\ 0.02813820
		\end{pmatrix}
	\end{eqnarray*}
	
	Результаты работы комбинированного метода:
	\begin{eqnarray*}
		& \lambda_1 = 57.14051354: e_1 = \begin{pmatrix}
			-0.01196465	\\ 0.15375126	\\ -0.03300425	\\ 0.98748575
		\end{pmatrix}; \lambda_2 = 99.01335045: e_2 = \begin{pmatrix}
			0.95817599	\\ 0.28037551	\\ -0.04647068	\\ -0.03359803
		\end{pmatrix}; \\
		& \lambda_3 = 108.74449211: e_3 = \begin{pmatrix}
			-0.28329838	\\ 0.94679111	\\ -0.01942462	\\ -0.15149686
		\end{pmatrix}; \lambda_4 = -160.29835612: e_4 = \begin{pmatrix}
			-0.03869947	\\ -0.03656110	\\ -0.99818529	\\ -0.02813820
		\end{pmatrix}.
	\end{eqnarray*}
	
Оценка эффективности QR-алгоритма

\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Метод}        & \textbf{{Хессенберг+Сдвиг}}&\textbf{{Хессенберг}}&\textbf{{Сдвиг}}&\textbf{{-}} \\ \hline
		Кол-во итераций &6&111&7&91           \\ \hline
			Кол-во операций &107+433&107+7617&765&9275           \\ \hline
	\end{tabular}
\end{center}
	
	\newpage
	\subsection{Тест 2 (несимметричная матрица)}
		\[
	\large A = \begin{pmatrix}
		1 & 2 & 4 & 8 \\
		3 & 2 & 5 & 9 \\
		4 & 5 & 1 & 3 \\
		7 & 6 & 4 & 5
	\end{pmatrix}
	\]
	\\\\
	Точный результат (вычисленный в Wolfram Mathematica):
	\begin{eqnarray*}
		& \lambda_1 = 17.902099820455 \qquad \Longleftrightarrow \qquad e_1 = \begin{pmatrix}
			0.724593586004\\ 0.895697899626\\ 0.613939330165\\1.000000000000
		\end{pmatrix};\\\\
		& \lambda_2 = -6.763156386957 \qquad \Longleftrightarrow \qquad e_2 = \begin{pmatrix}
			-1.251099077649\\-1.179485159695\\ 1.017862028691\\ 1.000000000000
		\end{pmatrix};\\\\
		& \lambda_3 = -1.562927995541 \qquad \Longleftrightarrow \qquad e_3 = \begin{pmatrix}
			-0.447030936159\\ 0.855145114104\\-2.141145531763\\ 1.000000000000
		\end{pmatrix};\\\\
		& \lambda_4 = -0.576015437956 \qquad \Longleftrightarrow \qquad e_4 = \begin{pmatrix}
			1.029162539057\\ -0.789544290409\\ 2.010721867225\\ 1.000000000000
		\end{pmatrix}.
	\end{eqnarray*}
	\newpage
		\qquad \qquad \qquad Исходная матрица $A$ \qquad Транспонированная матрица $A$ 
			\begin{eqnarray*}
			\lambda_1 = -0.5755956637   & \lambda_1^*=-0.5760057752\\
			e_1 = \begin{pmatrix}
				0.39684407\\ -0.30444754\\ -0.77533216\\0.38559891
			\end{pmatrix}; & e_1^* = \begin{pmatrix}
			0.64061807\\-0.66103775\\ -0.26269859\\ 0.28918338
		\end{pmatrix}; \\\\
			\lambda_2 = -1.5633429842  & \lambda_2^*=-1.5629370408\\
			  e_2 = \begin{pmatrix}
				0.17512983\\-0.33501346\\ 0.83881945\\ -0.39176199
			\end{pmatrix}; & e_2^* = \begin{pmatrix}
			0.66723473\\ -0.55127768\\ 0.43139107\\-0.25454368
		\end{pmatrix};\\\\
			\lambda_3 = -6.7631615068 & \lambda_3^* = -6.7631722638\\
			 e_3 = \begin{pmatrix}
				0.55992990	\\ 0.52787906\\-0.45554456\\ -0.44755040
			\end{pmatrix}; & e_3:* = \begin{pmatrix}
			0.53872703\\ 0.46482522\\-0.23594038\\ -0.66184805
		\end{pmatrix};\\\\
			 \lambda_4 = 17.9021001547 & \lambda_4^* = 17.9021150799\\
			  e_4 = \begin{pmatrix}
				0.44062842\\ 0.54467767\\ 0.3733391\\ 0.6081042
			\end{pmatrix} & e_4^* = \begin{pmatrix}
			0.44940459\\ 0.43463357\\ 0.39432338\\ 0.67352672
		\end{pmatrix}.
		\end{eqnarray*}
	\\

	\centering 
	Оценка эффективности QR-алгоритма
	
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Метод}        & \textbf{{Хессенберг+Сдвиг}}&\textbf{{Хессенберг}}&\textbf{{Сдвиг}}&\textbf{{-}} \\ \hline
			Кол-во итераций $A$ &7&11&7&11\\
			 \hline
			 Кол-во итераций $A^\text{T}$ &5&11&7&11\\
			 \hline
		\end{tabular}
	\end{center}


	\newpage
	\begin{thebibliography}{1}
		\bibitem{galanin} \textit{Галанин М.П., Савенков Е.Б.} Методы численного анализа математических\\ моделей. М.: Изд-во МГТУ им. Н.Э. Баумана,	2010. 592 с.
		
	\end{thebibliography}
	
	
\end{document}